<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Squid on Sajal Kayan </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://www.sajalkayan.com/tag/squid.xml</link>
    <language>en-us</language>
    
    
    <updated>Sat, 28 Jun 2008 05:26:21 &#43;0000</updated>
    
    <item>
      <title>Make your own cheap charlie CDN</title>
      <link>http://www.sajalkayan.com/make-your-own-cheap-charlie-cdn.html</link>
      <pubDate>Sat, 28 Jun 2008 05:26:21 &#43;0000</pubDate>
      
      <guid>http://www.sajalkayan.com/make-your-own-cheap-charlie-cdn.html</guid>
      <description>&lt;a href=&#34;http://en.wikipedia.org/wiki/Content_Delivery_Network&#34; target=&#34;_self&#34;&gt;CDN&lt;/a&gt; stands for Content Delivery(or Distribution) Network. It is a network of servers usually located in various geographic locations to improve the availability and access speed of a website (or webapp or other media). The main use of CDNs were during the nineteens when inter-continental access was slow, scarce and expensive. Origin server could be in the silicon valley, users from UK would access the node located in UK, so in theory only once the page would be downloaded from the US server to the UK server. Thus allowing the UK visitors to access the page locally resulting in a huge saving in inter-continental bandwidth costs and improved access times for the end users.

CDNs are traditionally a very expensive solution to implement if using any of the &lt;a href=&#34;http://en.wikipedia.org/wiki/Content_Delivery_Network#Commercial_CDNs&#34;&gt;established providers&lt;/a&gt;. The solution in itself is not very complex. I am in the process of implementing my very own CDN. The benefits are simple.
&lt;ul&gt;
	&lt;li&gt;It would reduce load on the origin server&lt;/li&gt;
	&lt;li&gt;Faster access if user downloads pages from a server closer to them&lt;/li&gt;
	&lt;li&gt;Since load on origin server is low, faster access even if cache needs to be refreshed&lt;/li&gt;
&lt;/ul&gt;
Some major portions of the CDN I am looking to implement
&lt;ol&gt;
	&lt;li&gt;&lt;strong&gt;Origin Server(s)&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;Geo targeting DNS servers&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;Squid Cache&lt;/strong&gt; - Set as Reverse Proxy or web accelerator&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;strong&gt;Origin Server&lt;/strong&gt; : This at the moment is a single server, which may be increased to run mysql and apache on separate boxes to increase productivity. This is up and running and in production.

&lt;strong&gt;Geo targeting DNS servers&lt;/strong&gt; : A perl script making use of Geo::IP and Net::DNS::Nameserver modules to resolve the query based on the origin country of the requester. The DNS script is in very early development. At the moment it is basically the example usage of Net::DNS::Nameserver with the Geo::IP loosely implemented. Need to implement some way of using config files which flush every 10 minutes, so I can use a series of servers runing this script and changes in config can be done on one server and rsynced across all nameservers running this script. The perl script is attached in the end. It would resolve foo.example.com differently if the request came from Malaysia.
&lt;strong&gt;
Squid Cache&lt;/strong&gt; : Squid cache is an Free proxy server which can be setup as a Reverse proxy. Users would query this proxy for pages and the proxy would deliver content, flushing the cache based on the rules defined in the squid.conf file and/or the expires headers tag set by the origin server. It can be setup such that different filetypes are cached in a different manner. Different url patterns need to be cached differently. Queries to some URL patterns by logged-in(based on cookies) users should be direct from origin server. These configurations are a little complicated. My plan for this configuration is attached in the end.

The Geo Dns and Squid would be installed on cheap VPS in a few countries. Will start with one to see how well it scales.

EDIT 1 : Playing with &lt;a href=&#34;http://varnish.projects.linpro.no/&#34;&gt;Varnish&lt;/a&gt; at the moment, considering it over squid.

The Geo Dns perl script :
&lt;pre lang=&#34;perl&#34;&gt;#!/usr/bin/perl

use Geo::IP;
use Net::DNS::Nameserver;
use strict;
use warnings;

sub reply_handler {
my ($qname, $qclass, $qtype, $peerhost,$query) = @_;
my ($rcode, @ans, @auth, @add);

my $gi = Geo::IP-&amp;gt;new(GEOIP_STANDARD);
print &#34;Received query from $peerhost\n&#34;;
my $ip =  substr $peerhost, 7;
my $country = $gi-&amp;gt;country_code_by_addr($ip);
print &#34;--$ip--$country \n\n&#34;;
$query-&amp;gt;print;

if ($qtype eq &#34;A&#34; &amp;amp;&amp;amp; $qname eq &#34;foo.example.com&#34; ) {
my ($ttl, $rdata) = (3600, &#34;10.1.2.3&#34;);
if ($country eq &#34;MY&#34; ) {
$rdata = &#34;10.1.2.4&#34;;
}
push @ans, Net::DNS::RR-&amp;gt;new(&#34;$qname $ttl $qclass $qtype $rdata&#34;);
$rcode = &#34;NOERROR&#34;;
}elsif( $qname eq &#34;foo.example.com&#34; ) {
$rcode = &#34;NOERROR&#34;;

}else{
$rcode = &#34;NXDOMAIN&#34;;
}

# mark the answer as authoritive (by setting the &#39;aa&#39; flag
return ($rcode, \@ans, \@auth, \@add, { aa =&amp;gt; 1 });
}

my $ns = Net::DNS::Nameserver-&amp;gt;new(
LocalPort    =&amp;gt; 53,
ReplyHandler =&amp;gt; \&amp;amp;reply_handler,

Verbose      =&amp;gt; 1,
) || die &#34;couldn&#39;t create nameserver object\n&#34;;

$ns-&amp;gt;main_loop;&lt;/pre&gt;
My idea for squid.conf :
&lt;pre lang=&#34;html&#34;&gt;1) Forward proxy :-

Allow following IPs to browse any website without any caching... allow https also...

a.b.c.d
w.x.y.z
127.0.0.1 (ill do a ssh tunnel)

2) Reverse proxy :-

Rules (to be followed serially, if rule 3 and 5 both match, rule 3 should be used):-

1) All urls ending in the following must be cached for minimum 5 days or expires headder. dont even check to see if file has updated.

.jpg, .gif, .css, .js, .swf, .png  (not case sensitive )

2) POST should never be cached

3) Few URLs should be cached for 30 mins (minimum/maximum) no matter what the expires headder says.

http://www.mysite.com/urla/
http://www.mysite.com/urlc.html
etc...

4) Requests to http://www.mysite.com/sectiona/* :-

* If user has following cookies pass them direct hint : &#34;acl cookie_test req_header Cookie ﻿^.*(comment_author_|wordpress|wp-postpass_).*$&#34;
* http://www.mysite.com/sectiona/sub-section1/* : 30 mins cache no matter what!
* If requester is Googlebot : serve from cache only if it the copy in cache is 5 mins old. else update the cache.
* for other users cache urls ending in .html for 60 mins , rest for 20 mins

5) Requests to http://www.mysite.com/sectionb/*

* no cache unless images.
* Allow access only if user has a particular cookie e.g. secret_word=another_word

6) urls which are NOT in point 4 or 5 :-

* If users have cookie eg. no_cache then pass direct
* 5 min cache for http://www.mysite.com/sectionc/*
* Cache the shit out of everything else for 30 mins

Special Instructions : If origin server is unreachable then show cached result, no matter what. The first cache server is a VPS running ubuntu server with 128 megs of dedicated non-burstable ram and has 4.3 GB diskspace left. resources can be upgraded on request. Where I have mentioned &#34;no matter what&#34; i dont want the proxy server bothering the origin server at all. The origin and proxies will be located far geographically so connection between them may not be optimal.

In case squid allows for URL rewriting, i would like to also map for example :-
us.mysite.com -&amp;gt; www.mysite.com

so if user can access the same stuff by going to www.mysite.com or even us.mysite.com

Also if URL rewriting is possible in Squid, in the future id like to be able to map ... http://www.mysite.com/subfolder as http://somesite.com/subfolder and http://www.mysite.com/anotherfolder as http://anothersite.com/anotherfolder

Also.. if squid supports ssl, would it be possible to use https (and also install some certificate on the squid) then users connection to and from the proxy is encrypted if needed, but the connection between squid and origin server is plaintext ?&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Secure Proxy using Squid</title>
      <link>http://www.sajalkayan.com/secure-proxy-using-squid.html</link>
      <pubDate>Sat, 05 Jan 2008 05:16:52 &#43;0000</pubDate>
      
      <guid>http://www.sajalkayan.com/secure-proxy-using-squid.html</guid>
      <description>Had originally posted this on Friday, September 29th, 2006 during the peak of Takhsin&#39;s rampant blocking of websites and the blocking proxies were messing up often blocking un-blocked pages also.

It is very important to use a secure proxy if you browse the web from unsecure Networks.
The basic requirements are a Linux box in a location which is secure and another PC Linux or windows which may be in the unsecure location.

NOTE : Please excuse my newbieness. I do not provide any kind of waranty on this article, only hope that it may be useful or interesting to you.

For this article, the squid was setup on a CentOS based server and accessed from FC5 based notebook using Mozilla Firefox. The settings would be simillar for other distributions/ browsers.

This method is not to be used illegally to view government blocked websites. Please follow laws of the country you reside in.&lt;!--more--&gt;

On CentOS it is very easy to install squid using yum (note you must be root to continue with the instalation)
&lt;blockquote&gt;&lt;em&gt;yum install squid&lt;/em&gt;&lt;/blockquote&gt;
On Debian based system :-
&lt;blockquote&gt;&lt;em&gt;apt-get install squid&lt;/em&gt;&lt;/blockquote&gt;
For others/slackware :-

Download squid source file as a gzipped tar ball (squid-x.y-STABLE.tar.gz) available at http://www.squid-cache.org/ or from ftp://www.squid-cache.org/pub. Then run the following commands
&lt;blockquote&gt;&lt;em&gt;tar -xvzf squid-*-src.tar.gz&lt;/em&gt;
&lt;em&gt;cd squid -*&lt;/em&gt;
&lt;em&gt;./configure&lt;/em&gt;
&lt;em&gt;make&lt;/em&gt;
&lt;em&gt;make install&lt;/em&gt;&lt;/blockquote&gt;
This by default, will install in “/usr/local/squid”.

There are some basic configurations which you may need to edit.
By default the squid config file is located in /etc/squid/squid.conf on centos with squid installed using yum. Config file may be located at /usr/local/squid/etc/squid.conf if you have installed manually.

Start Squid on CentOS
&lt;blockquote&gt;&lt;em&gt;/etc/init.d/squid start&lt;/em&gt;&lt;/blockquote&gt;
Squid is now ready to serve as proxy only to local host and is listening on port 3128

Now you have squid listening to localhost only. We need to make a tunnel from the server to the PC.
Note for windows users reffer to &lt;a href=&#34;http://web.archive.org/web/20070505081807/http://www.jfitz.com/tips/putty_config.html&#34; target=&#34;_blank&#34; title=&#34;Secure tunnel on windows using putty &#34;&gt;putty configuration guide&lt;/a&gt;
to do this on a FC5 (Fedora core instructions may be similar for other distributions) be root(or superuser) and run the following command.
&lt;blockquote&gt;&lt;em&gt;ssh -L 8080:localhost:3128 someserver.com -l username&lt;/em&gt;&lt;/blockquote&gt;
Replace the someserver.com with the DNS name or IP of your newly setup proxy server.
In case the SSHD on your server listens to aa port other than port 22, you may need to enter the following command
&lt;blockquote&gt;&lt;em&gt;ssh -L 8080:localhost:3128 someserver.com -p 1022 -l username&lt;/em&gt;&lt;/blockquote&gt;
In the above example:-
8080 is a free unused port on your PC. We will tunnel this port to squid.
localhost is your PC
3128 is the port on the proxy server where squid is listening to.
someserver.com is your proxy servers hostname or IP address.
1022 is the port on your server where SSHD listens to.
username is your username on the server

After entering the ssh command, you will get a prompt asking for a password. Enter the password for ‘username’.

Open Mozilla Firefox
Click Edit the preferences. Then click on the box Connection Settings
Choose Manual proxy configuration.
Under HTTP proxy type in localhost. For port type in 8080 (change it as per your tunnel settingings)
Then click OK

You should now be able to use the Web browser to surf the net using browsing the web without fear of packets being sniffed.
This only encrypts the information exchanged between your workstation and the proxy server. The information from your proxy to the website you are browsing can still be sniffed, so to be on the safe side use https when browsing sites which support it.
</description>
    </item>
    
  </channel>
</rss>