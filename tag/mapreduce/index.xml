<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mapreduce on Sajal Kayan</title>
    <link>http://www.sajalkayan.com/tag/mapreduce.xml</link>
    <description>Recent content in Mapreduce on Sajal Kayan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://www.sajalkayan.com/tag/mapreduce.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>kubemr: Kubernetes native distributed MapReduce framework</title>
      <link>http://www.sajalkayan.com/post/kubemr.html</link>
      <pubDate>Thu, 20 Apr 2017 20:00:00 +0000</pubDate>
      
      <guid>http://www.sajalkayan.com/post/kubemr.html</guid>
      <description>

&lt;p&gt;tl;dr version &lt;a href=&#34;https://github.com/turbobytes/kubemr&#34;&gt;https://github.com/turbobytes/kubemr&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;Few years ago I wrote a &lt;a href=&#34;https://en.wikipedia.org/wiki/MapReduce&#34;&gt;MapReduce&lt;/a&gt; tool in Go called &lt;a href=&#34;https://github.com/turbobytes/gomr&#34;&gt;gomr&lt;/a&gt;. I never used it for anything, just wrote it as an experiment to see if I could run MapReduce jobs without requiring a master, and it worked. The distributed consensus is provided by etcd which is typically deployed as a cluster. I am not fond of master/slave or primary/secondary systems. I like it when individual units are responsible and co-ordinate with each other and do their fair share of work.&lt;/p&gt;

&lt;p&gt;gomr used S3 to upload user binaries and store map/reduce outputs and results. A running worker would query etcd for pending jobs, fetch binary from S3 and then &lt;code&gt;Exec()&lt;/code&gt; it.&lt;/p&gt;

&lt;p&gt;This week I re-wrote the tool to let &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; take care of a lot of the deployment hassles and called it &lt;a href=&#34;https://github.com/turbobytes/kubemr&#34;&gt;&lt;em&gt;kubemr&lt;/em&gt;&lt;/a&gt;. Yes, I am aware I suck at naming things.&lt;/p&gt;

&lt;h3 id=&#34;introducing-kubemr&#34;&gt;Introducing kubemr&lt;/h3&gt;

&lt;p&gt;kubemr is a MapReduce system that runs within a Kubernetes cluster. Apart from initial complexity of setting up the cluster and managing it, kubemr itself is pretty simple.&lt;/p&gt;

&lt;h4 id=&#34;getting-started&#34;&gt;Getting started&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/turbobytes/kubemr/blob/master/README.md&#34;&gt;https://github.com/turbobytes/kubemr/blob/master/README.md&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;json-patch&#34;&gt;JSON Patch&lt;/h4&gt;

&lt;p&gt;Originally I planned to use etcd for state, but instead, I decided to use &lt;a href=&#34;http://jsonpatch.com/&#34;&gt;JSON patch&lt;/a&gt; functionality &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#patch-operations&#34;&gt;provided by kubernetes&lt;/a&gt; to make changes to this state. The &lt;code&gt;test&lt;/code&gt; operation allows the patch to fail if some condition is not met.&lt;/p&gt;

&lt;p&gt;Example lock using JSON patch :-&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  { &amp;quot;op&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/lockholder&amp;quot;, &amp;quot;value&amp;quot;: None },
  { &amp;quot;op&amp;quot;: &amp;quot;add&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/lockholder&amp;quot;, &amp;quot;value&amp;quot;: &amp;quot;me&amp;quot; },
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above operation would fail if the &lt;code&gt;lockholder&lt;/code&gt; already has a value. So multiple users might try to acquire the lock at the same time but only 1 would succeed.&lt;/p&gt;

&lt;h4 id=&#34;job-state&#34;&gt;Job state&lt;/h4&gt;

&lt;p&gt;All state information about a MapReduce task is stored in a kubernetes ThirdPartyResource.&lt;/p&gt;

&lt;p&gt;In the case of kubemr, I named it &lt;code&gt;MapReduceJob&lt;/code&gt;. All I need to kick off some task is submit the following example yaml to Kubernetes.&lt;/p&gt;

&lt;div class=&#34;highlight&#34; style=&#34;background: #ffffff&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;apiVersion: &lt;span style=&#34;background-color: #fff0f0&#34;&gt;&amp;quot;turbobytes.com/v1alpha1&amp;quot;&lt;/span&gt;
kind: MapReduceJob
metadata:
  generateName: test-
spec:
  image: turbobytes/kubemr-wordcount
  replicas: 10
  inputs:
  - https://tools.ietf.org/rfc/rfc4501.txt
  - https://tools.ietf.org/rfc/rfc2017.txt
  - https://tools.ietf.org/rfc/rfc2425.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The image &lt;code&gt;turbobytes/kubemr-wordcount&lt;/code&gt; knows how to process this. I used &lt;code&gt;generateName&lt;/code&gt; instead of the usual &lt;code&gt;name&lt;/code&gt; to ask Kubernets to generate a random unique suffix.&lt;/p&gt;

&lt;p&gt;Once the job is complete, something like this is stamped onto the object.&lt;/p&gt;

&lt;div class=&#34;highlight&#34; style=&#34;background: #ffffff&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;results:
- s3://kubemr/kubemr/test-z0dk5/reduce/0.txt
- s3://kubemr/kubemr/test-z0dk5/reduce/1.txt
- s3://kubemr/kubemr/test-z0dk5/reduce/2.txt
- s3://kubemr/kubemr/test-z0dk5/reduce/3.txt
- s3://kubemr/kubemr/test-z0dk5/reduce/4.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;h4 id=&#34;lifecycle-of-a-mapreducejob&#34;&gt;Lifecycle of a MapReduceJob&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;User creates a &lt;a href=&#34;https://github.com/turbobytes/kubemr/blob/master/manifests/wordcount.yaml&#34;&gt;MapReduceJob object&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://github.com/turbobytes/kubemr/blob/master/manifests/operator-deployment.yaml&#34;&gt;operator&lt;/a&gt; sees there is no &lt;code&gt;status&lt;/code&gt; field, it validates the schema and either marks it as &lt;code&gt;FAIL&lt;/code&gt; if invalid or &lt;code&gt;PENDING&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;An operator marks status as &lt;code&gt;DEPLOYING&lt;/code&gt; if status is &lt;code&gt;PENDING&lt;/code&gt;. This guarantees a lock to a single operator instance (in case multiple are running).&lt;/li&gt;
&lt;li&gt;The operator creates desired &lt;code&gt;ConfigMap&lt;/code&gt; and &lt;code&gt;Secret&lt;/code&gt; objects needed for the &lt;code&gt;Job&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The operator &lt;a href=&#34;https://github.com/turbobytes/kubemr/blob/4a19b75819f57bcecf0dfcb0d69eda1070f7dbbc/pkg/job/client.go#L164&#34;&gt;creates the Kubernetes Job object&lt;/a&gt; and marks status as &lt;code&gt;DEPLOYED&lt;/code&gt;. It creates n worker pods where n is determined by &lt;code&gt;replicas&lt;/code&gt; field in the manifest in step #1. In this example it is 10.&lt;/li&gt;
&lt;li&gt;As workers come online, they pick a map task, on successfully acquiring a lock they execute it and populate the &lt;code&gt;outputs&lt;/code&gt; field. Each output belongs to a particular &lt;em&gt;partition&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Once all map tasks are complete, a worker creates reduce stage specification in the &lt;code&gt;MapReduceJob&lt;/code&gt;. Only a single worker would succeed in doing so because we use &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#patch-operations&#34;&gt;JSON patch&lt;/a&gt;. A single reduce task corresponds to a partition created during map phase with all outputs belonging to that partition used as input for the reduce.&lt;/li&gt;
&lt;li&gt;The workers start executing reduce stage.&lt;/li&gt;
&lt;li&gt;When all reduce tasks are completed successfully, a worker aggregates all reduce outputs into a list saves it as &lt;code&gt;results&lt;/code&gt; and marks the &lt;code&gt;MapReduceJob&lt;/code&gt; as &lt;code&gt;COMPLETE&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If any thing fails along the way, the entire &lt;code&gt;MapReduceJob&lt;/code&gt; is marked as &lt;code&gt;FAIL&lt;/code&gt;. &lt;em&gt;Do or do not, there is no try.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The inputs and outputs here are just strings. We do provide utility methods to fetch/store files as S3 objects, but the meaning of the string is up to the user.&lt;/p&gt;

&lt;h4 id=&#34;components&#34;&gt;Components&lt;/h4&gt;

&lt;h5 id=&#34;operator&#34;&gt;Operator&lt;/h5&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/turbobytes/kubemr/tree/master/cmd/operator&#34;&gt;https://github.com/turbobytes/kubemr/tree/master/cmd/operator&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The operator watches for &lt;code&gt;MapReduceJob&lt;/code&gt;s and creates Kubernetes resources for it&lt;/p&gt;

&lt;h5 id=&#34;worker&#34;&gt;Worker&lt;/h5&gt;

&lt;p&gt;Example: &lt;a href=&#34;https://github.com/turbobytes/kubemr/tree/master/cmd/wordcount&#34;&gt;https://github.com/turbobytes/kubemr/tree/master/cmd/wordcount&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is user supplied docker image. It must have &lt;code&gt;CMD&lt;/code&gt; or &lt;code&gt;ENTRYPOINT&lt;/code&gt; defined. The actual code needs to basically satisfy &lt;a href=&#34;https://godoc.org/github.com/turbobytes/kubemr/pkg/worker#JobWorker&#34;&gt;JobWorker interface&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The user could actually write the worker in any language, as long as they implement everything that the Go library does. Perhaps in future I could provide wrappers to allow workers to be written as shell commands.&lt;/p&gt;

&lt;h5 id=&#34;api-server&#34;&gt;API server&lt;/h5&gt;

&lt;p&gt;TODO: Not yet implemented.&lt;/p&gt;

&lt;p&gt;Will probably have some way to stream results, and some way to retry failed jobs, etc.&lt;/p&gt;

&lt;h4 id=&#34;differences-from-gomr&#34;&gt;Differences from gomr&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;Worker code is shipped as user generated docker images&lt;/li&gt;
&lt;li&gt;Kubernetes apiserver(which itself is backed by etcd) is used for consensus. This means I don&amp;rsquo;t have to make maintain tooling to create job objects.&lt;/li&gt;
&lt;li&gt;When a new job comes in, we create kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/&#34;&gt;batch jobs&lt;/a&gt; to run the user-provided docker images.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;This is pre-pre-pre-alpha. Use at your own risk. There may be backwards-incompatible changes.&lt;/li&gt;
&lt;li&gt;I am not cleaning up after myself properly. If playing with it in a production cluster, do everything in a new namespace and simply delete the namespace once done.&lt;/li&gt;
&lt;li&gt;I have not retrying any failed tasks. Probably not fine to put a 10GB job only to find out some intermittent DNS timeout broke the job.&lt;/li&gt;
&lt;li&gt;I don&amp;rsquo;t know what guarantees the kube-apiserver provides about the JSON Patch. Especially in multi-master environments.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I had fun building this. Hope you have fun playing with it too.&lt;/p&gt;

&lt;p&gt;Looking forward to using this at &lt;a href=&#34;http://www.turbobytes.com/&#34;&gt;work&lt;/a&gt; in the near future.&lt;/p&gt;

&lt;p&gt;Special thanks to &lt;a href=&#34;http://arschles.com/&#34;&gt;Aaron Schlesinger&lt;/a&gt;. His &lt;a href=&#34;https://www.youtube.com/watch?v=qiB4RxCDC8o&#34;&gt;talk&lt;/a&gt; and the corresponding &lt;a href=&#34;https://github.com/arschles/2017-KubeCon-EU&#34;&gt;example code&lt;/a&gt; helped a lot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Disco &#43; EC2 spot instance = WIN</title>
      <link>http://www.sajalkayan.com/disco-ec2-spot-instance-win.html</link>
      <pubDate>Tue, 30 Oct 2012 13:13:59 +0000</pubDate>
      
      <guid>http://www.sajalkayan.com/disco-ec2-spot-instance-win.html</guid>
      <description>&lt;p&gt;tl;dr version : &lt;a href=&#34;https://github.com/sajal/disposabledisco&#34;&gt;This&lt;/a&gt; is how I spent a Saturday evening.&lt;/p&gt;

	&lt;p&gt;&amp;lt;blink&amp;gt;&lt;strong&gt;Warning&lt;/strong&gt;: If you like to write Java, stop reading now. Go back to using Hadoop. It&#39;s a much more mature project.&amp;lt;/blink&amp;gt;&lt;/p&gt;

	&lt;p&gt;As a part of my &lt;a href=&#34;http://www.turbobytes.com/&#34; title=&#34;multi-cdn service&#34;&gt;job&lt;/a&gt;, I do a lot of number processing. Over the course of last few weeks, I shifted to doing most of it using &lt;a href=&#34;http://research.google.com/archive/mapreduce.html&#34;&gt;MapReduce&lt;/a&gt; using &lt;a href=&#34;http://discoproject.org/&#34;&gt;Disco&lt;/a&gt;. Its a wonderful approach to processing big data where the time to process data is directly proportional to the amount of hardware you throw at it and the quantity of data. The amount of data to be processed can (in theory) be unlimited. While I don&#39;t do anything of Google scale, I deal with &lt;em&gt;Small Big Data&lt;/em&gt;. My datasets for an individual job would probably not exceed 1 GB. I can currently afford to continue not use MapReduce, but as my data set grows, I would &lt;em&gt;have to&lt;/em&gt; do distributed computing, so better start early.&lt;/p&gt;

	&lt;h3&gt;Getting started with Disco&lt;/h3&gt;

	&lt;p&gt;If you, like me, had given up on MapReduce in the past after trying to deal with administrating Hadoop, now is a great time to look into Disco. Installation is pretty easy. &lt;a href=&#34;http://discoproject.org/doc/disco/start/download.html&#34;&gt;Follow the docs&lt;/a&gt;. Within 5 minutes I was writing Jobs in python to process data, would have been faster if I knew before-hand that SSH daemon should be listening on port 22.&lt;/p&gt;

	&lt;p&gt;Python for user scripts + Erlang for backend == match made in heaven&lt;/p&gt;

	&lt;h3&gt;Enter disposable Disco&lt;/h3&gt;

	&lt;p&gt;I made a &lt;a href=&#34;https://github.com/sajal/disposabledisco&#34;&gt;set of python scripts&lt;/a&gt; to launch and manage Disco clusters on EC2 where there is no need for any data to be stored. In my usecase, the input is read from Amazon S3 and output goes back into S3.

	&lt;p&gt;There are some issues with running disco on EC2.&lt;/p&gt;

	&lt;ul&gt;
		&lt;li&gt;Must have ssh/keys setup such that Master can ssh into slaves.&lt;/li&gt;
		&lt;li&gt;Must have a file with &lt;em&gt;erlang cookie&lt;/em&gt; with same contents on all slaves&lt;/li&gt;
		&lt;li&gt;Must inform master the &lt;em&gt;hostnames&lt;/em&gt; of the slaves. FQDN or anything with a dot gets rejected&lt;/li&gt;
		&lt;li&gt;The default root directories have very limited storage space, usually 8GB&lt;/li&gt;
	&lt;/ul&gt;

	&lt;p&gt;disposabledisco takes care of the above things and more. Everything needed to run the cluster is defined in a config file. First generate a sample config file.&lt;/p&gt;

	&lt;pre&gt;
python create_config.py &gt; config.json
	&lt;/pre&gt;

	&lt;p&gt;This creates a new file with some pre-populated values. For my case the config file looks like this(some info masked)&lt;/p&gt;

	&lt;pre style=&#34;width:500;overflow-x:scroll;&#34;&gt;
{
    &#34;AWS_SECRET&#34;: &#34;SNIPPED&#34;, 
    &#34;ADDITIONAL_PACKAGES&#34;: [
        &#34;git&#34;, 
        &#34;libwww-perl&#34;, 
        &#34;mongodb-clients&#34;, 
        &#34;python-numpy&#34;, 
        &#34;python-scipy&#34;, 
        &#34;libzmq-dev&#34;, 
        &#34;s3cmd&#34;, 
        &#34;ntp&#34;, 
        &#34;libguess1&#34;, 
        &#34;python-dnspython&#34;, 
        &#34;python-dateutil&#34;, 
        &#34;pigz&#34;
    ], 
    &#34;SLAVE_MULTIPLIER&#34;: 1, 
    &#34;PIP_REQUIREMENTS&#34;: [
        &#34;iso8601&#34;,
        &#34;pygeoip&#34;
    ], 
    &#34;MASTER_MULTIPLIER&#34;: 1, 
    &#34;MGMT_KEY&#34;: &#34;ssh-rsa SNIPPED\n&#34;, 
    &#34;SECURITY_GROUPS&#34;: [&#34;disco&#34;], 
    &#34;BASE_PACKAGES&#34;: [
        &#34;python-pip&#34;, 
        &#34;python-dev&#34;, 
        &#34;lighttpd&#34;
    ], 
    &#34;TAG_KEY&#34;: &#34;disposabledisco&#34;, 
    &#34;NUM_SLAVES&#34;: 30, 
    &#34;KEY_NAME&#34;: &#34;SNIPPED&#34;, 
    &#34;AWS_ACCESS&#34;: &#34;SNIPPED&#34;, 
    &#34;INSTANCE_TYPE&#34;: &#34;c1.medium&#34;, 
    &#34;AMI&#34;: &#34;ami-6d3f9704&#34;, 
    &#34;MAX_BID&#34;: &#34;0.04&#34;,
    &#34;POST_INIT&#34;: &#34;echo \&#34;[default]\naccess_key = SNIPPED\nsecret_key = SNIPPED\&#34; &gt; /tmp/s3cfg\ncd /tmp\ns3cmd -c /tmp/s3cfg get s3://SNIPPED/GeoIPASNum.dat.gz\ns3cmd -c /tmp/s3cfg get s3://SNIPPED/GeoIP.dat.gz\ns3cmd -c /tmp/s3cfg get s3://SNIPPED/GeoLiteCity.dat.gz\ns3cmd -c /tmp/s3cfg get s3://SNIPPED/GeoIPRegion.dat.gz\ngunzip *.gz\nchown disco:disco *.dat\n\n&#34;
}
	&lt;/pre&gt;

	&lt;p&gt;This tells disposabledisco that I want a cluster with 1 master and 30 slaces all of type &lt;em&gt;c1.medium&lt;/em&gt;, and use &lt;em&gt;ami-6d3f9704&lt;/em&gt; as the starting point. It lists out the packages to be installed via apt-get and python dependencies to be installed using PIP. You can link to external tar, git repo, etc. Basically anything pip allows after &lt;em&gt;pip install&lt;/em&gt;&lt;/p&gt;

	&lt;p&gt;The &lt;em&gt;POST_INIT&lt;/em&gt; portion is bash script that runs as root after rest of the install. In my case I am downloading and uncompressing different GeoIP databases archived in a S3 bucket for use from within disco jobs.&lt;/p&gt;

	&lt;p&gt;Once the config file is ready run the following command many times. The output is fairly verbose.&lt;/p&gt;

	&lt;pre&gt;
python create_cluster.py config.json
	&lt;/pre&gt;

	&lt;p&gt;Why many times? Cause there is no state stored in the system. All state is managed using EC2 tags. This is what the script does on each run&lt;/p&gt;

	&lt;ul&gt;
		&lt;li&gt;Check if master is running. If not request a spot instance for it (and kill any zombie slaves lying around from previous runs).&lt;/li&gt;
		&lt;li&gt;
			If master us up and running.
			&lt;ul&gt;
				&lt;li&gt;Print the ssh command needed to setup port forwarding. After running the given ssh command you can see http://localhost:8090 on the browser to see disco&#39;s UI in all its glory.&lt;/li&gt;
				&lt;li&gt;print the command to export DISCO_PROXY so you can create jobs locally&lt;/li&gt;
				&lt;li&gt;Check inventory of slaves. A slave can have 3 statuses. 1) &lt;em&gt;pending&lt;/em&gt; - spot instance requested. 2) &lt;em&gt;running&lt;/em&gt; - the instance is running. 3) &lt;em&gt;bootstrapped&lt;/em&gt; - slave is completely setup and can be added to master.&lt;/li&gt;
				&lt;li&gt;If total number of slaves is less than &lt;em&gt;NUM_SLAVES&lt;/em&gt; launch the remaining&lt;/li&gt;
				&lt;li&gt;Try and bootstrap any &lt;em&gt;running&lt;/em&gt; instances. If bootstrap was successful, change the EC2 tag.&lt;/li&gt;
			&lt;/ul&gt;
		&lt;/li&gt;
		&lt;li&gt;Finally, update the master&#39;s disco config. Telling it hostnames of instances to use and number of workers.&lt;/li&gt;
		&lt;li&gt;???&lt;/li&gt;
		&lt;li&gt;Profit&lt;/li&gt;
	&lt;/ul&gt;

&lt;img src=&#34;http://i.ticdn.com/sajal/disco-cluster-small.png&#34; width=&#34;500&#34; height=&#34;377&#34; alt=&#34;Cloudwatch showing 31 instances&#34; title=&#34;Cloudwatch showing 31 instances&#34; /&gt;

	&lt;p&gt;Many steps involve EC2 provisioning spot instances, waiting for instance to get initialized, etc..&lt;/p&gt;

	&lt;p&gt;To help with shipping output to S3, I made some output classes for Disco&lt;/p&gt;

	&lt;ul&gt;
		&lt;li&gt;&lt;a href=&#34;https://gist.github.com/3919506&#34;&gt;S3Output&lt;/a&gt; - Each key, value returned creates a new file in S3 with the key as S3 key and value as String thats dumped inside it. So, one key should be yielded only once from reduce.&lt;/li&gt;
		&lt;li&gt;&lt;a href=&#34;https://gist.github.com/3975607&#34;&gt;S3LineOutput&lt;/a&gt; Similar to S3Output, but now it stores the output, and joins the output as one big file. has options for sorting, unique, etc. &lt;/li&gt;
	&lt;/ul&gt;

	&lt;p&gt;Both these functions can be configured gzip the contents before uploading.&lt;/p&gt;

	&lt;p&gt;As far as input is concerned, I send it a list of signed S3 urls. (Sidenote: It seems disco cannot handle https inputs at the moment, so I use http). A sample job run might look like.. &lt;/p&gt;

	&lt;pre style=&#34;width:500;overflow-x:scroll;&#34;&gt;
def get_urls():
    urls = []
    for k in bucket.list(prefix=&#34;processed&#34;):
        if k.name.endswith(&#34;gz&#34;):
            urls += [k.generate_url(3660).replace(&#34;https&#34;, &#34;http&#34;)]
    return urls

MyExampleJob().run(
	input=get_urls(),
	params={
		&#34;AWS_KEY&#34;: &#34;SNIP&#34;,
		&#34;AWS_SECRET&#34;: &#34;SNIP&#34;,
		&#34;BUCKET_NAME&#34;: &#34;SNIP&#34;,
		&#34;gzip&#34;: True
	},
	partitions=10,
	required_files=[&#34;s3lineoutput.py&#34;],
	reduce_output_stream=s3_line_output_stream
	).wait()

	&lt;/pre&gt;

	&lt;p&gt;Bonus - &lt;a href=&#34;https://gist.github.com/3941935&#34;&gt;MagicList&lt;/a&gt; - Memory efficient way to store and process potentially infinite lists.&lt;/p&gt;

	&lt;p&gt;We used Disco to compute numbers for a series of blogposts on &lt;a href=&#34;http://www.cdnplanet.com/&#34;&gt;CDN Planet&lt;/a&gt;. For this analysis it was painful process for me to manually launch Disco clusters, which lead me to create the helper scripts.&lt;/p&gt;
	&lt;ul&gt;
		&lt;li&gt;Part 1 : &lt;a href=&#34;http://www.cdnplanet.com/blog/google-dns-opendns-and-cdn-performance/&#34;&gt;Google DNS, OpenDNS and CDN performance&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;Part 2 : &lt;a href=&#34;http://www.cdnplanet.com/blog/which-cdns-support-edns-client-subnet/&#34;&gt;Which CDNs support edns-client-subnet?&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;Part 3 : &lt;a href=&#34;http://www.cdnplanet.com/blog/real-world-cdn-performance-googledns-opendns-users/&#34;&gt;Real-world CDN performance for Google DNS and OpenDNS users&lt;/a&gt;&lt;/li&gt;
	&lt;/ul&gt;

	&lt;h3&gt;Shameless plug&lt;/h3&gt;

	&lt;a href=&#34;http://www.turbobytes.com/&#34;&gt;&lt;strong&gt;Turbobytes, multi-CDN made easy&lt;/strong&gt;&lt;/a&gt;

	&lt;p&gt;Have your static content delivered by 6 global content delivery networks, not just 1. Turbobytes&#39; platform closely monitors CDN performance and makes sure your content is always delivered by the fastest CDN, automatically.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>