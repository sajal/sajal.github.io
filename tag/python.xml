<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Python on Sajal Kayan </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://www.sajalkayan.com/tag/python.xml</link>
    <language>en-us</language>
    
    
    <updated>Tue, 30 Oct 2012 13:13:59 &#43;0000</updated>
    
    <item>
      <title>Disco &#43; EC2 spot instance = WIN</title>
      <link>http://www.sajalkayan.com/disco-ec2-spot-instance-win.html</link>
      <pubDate>Tue, 30 Oct 2012 13:13:59 &#43;0000</pubDate>
      
      <guid>http://www.sajalkayan.com/disco-ec2-spot-instance-win.html</guid>
      <description>&lt;p&gt;tl;dr version : &lt;a href=&#34;https://github.com/sajal/disposabledisco&#34;&gt;This&lt;/a&gt; is how I spent a Saturday evening.&lt;/p&gt;

	&lt;p&gt;&amp;lt;blink&amp;gt;&lt;strong&gt;Warning&lt;/strong&gt;: If you like to write Java, stop reading now. Go back to using Hadoop. It&#39;s a much more mature project.&amp;lt;/blink&amp;gt;&lt;/p&gt;

	&lt;p&gt;As a part of my &lt;a href=&#34;http://www.turbobytes.com/&#34; title=&#34;multi-cdn service&#34;&gt;job&lt;/a&gt;, I do a lot of number processing. Over the course of last few weeks, I shifted to doing most of it using &lt;a href=&#34;http://research.google.com/archive/mapreduce.html&#34;&gt;MapReduce&lt;/a&gt; using &lt;a href=&#34;http://discoproject.org/&#34;&gt;Disco&lt;/a&gt;. Its a wonderful approach to processing big data where the time to process data is directly proportional to the amount of hardware you throw at it and the quantity of data. The amount of data to be processed can (in theory) be unlimited. While I don&#39;t do anything of Google scale, I deal with &lt;em&gt;Small Big Data&lt;/em&gt;. My datasets for an individual job would probably not exceed 1 GB. I can currently afford to continue not use MapReduce, but as my data set grows, I would &lt;em&gt;have to&lt;/em&gt; do distributed computing, so better start early.&lt;/p&gt;

	&lt;h3&gt;Getting started with Disco&lt;/h3&gt;

	&lt;p&gt;If you, like me, had given up on MapReduce in the past after trying to deal with administrating Hadoop, now is a great time to look into Disco. Installation is pretty easy. &lt;a href=&#34;http://discoproject.org/doc/disco/start/download.html&#34;&gt;Follow the docs&lt;/a&gt;. Within 5 minutes I was writing Jobs in python to process data, would have been faster if I knew before-hand that SSH daemon should be listening on port 22.&lt;/p&gt;

	&lt;p&gt;Python for user scripts + Erlang for backend == match made in heaven&lt;/p&gt;

	&lt;h3&gt;Enter disposable Disco&lt;/h3&gt;

	&lt;p&gt;I made a &lt;a href=&#34;https://github.com/sajal/disposabledisco&#34;&gt;set of python scripts&lt;/a&gt; to launch and manage Disco clusters on EC2 where there is no need for any data to be stored. In my usecase, the input is read from Amazon S3 and output goes back into S3.

	&lt;p&gt;There are some issues with running disco on EC2.&lt;/p&gt;

	&lt;ul&gt;
		&lt;li&gt;Must have ssh/keys setup such that Master can ssh into slaves.&lt;/li&gt;
		&lt;li&gt;Must have a file with &lt;em&gt;erlang cookie&lt;/em&gt; with same contents on all slaves&lt;/li&gt;
		&lt;li&gt;Must inform master the &lt;em&gt;hostnames&lt;/em&gt; of the slaves. FQDN or anything with a dot gets rejected&lt;/li&gt;
		&lt;li&gt;The default root directories have very limited storage space, usually 8GB&lt;/li&gt;
	&lt;/ul&gt;

	&lt;p&gt;disposabledisco takes care of the above things and more. Everything needed to run the cluster is defined in a config file. First generate a sample config file.&lt;/p&gt;

	&lt;pre&gt;
python create_config.py &gt; config.json
	&lt;/pre&gt;

	&lt;p&gt;This creates a new file with some pre-populated values. For my case the config file looks like this(some info masked)&lt;/p&gt;

	&lt;pre style=&#34;width:500;overflow-x:scroll;&#34;&gt;
{
    &#34;AWS_SECRET&#34;: &#34;SNIPPED&#34;, 
    &#34;ADDITIONAL_PACKAGES&#34;: [
        &#34;git&#34;, 
        &#34;libwww-perl&#34;, 
        &#34;mongodb-clients&#34;, 
        &#34;python-numpy&#34;, 
        &#34;python-scipy&#34;, 
        &#34;libzmq-dev&#34;, 
        &#34;s3cmd&#34;, 
        &#34;ntp&#34;, 
        &#34;libguess1&#34;, 
        &#34;python-dnspython&#34;, 
        &#34;python-dateutil&#34;, 
        &#34;pigz&#34;
    ], 
    &#34;SLAVE_MULTIPLIER&#34;: 1, 
    &#34;PIP_REQUIREMENTS&#34;: [
        &#34;iso8601&#34;,
        &#34;pygeoip&#34;
    ], 
    &#34;MASTER_MULTIPLIER&#34;: 1, 
    &#34;MGMT_KEY&#34;: &#34;ssh-rsa SNIPPED\n&#34;, 
    &#34;SECURITY_GROUPS&#34;: [&#34;disco&#34;], 
    &#34;BASE_PACKAGES&#34;: [
        &#34;python-pip&#34;, 
        &#34;python-dev&#34;, 
        &#34;lighttpd&#34;
    ], 
    &#34;TAG_KEY&#34;: &#34;disposabledisco&#34;, 
    &#34;NUM_SLAVES&#34;: 30, 
    &#34;KEY_NAME&#34;: &#34;SNIPPED&#34;, 
    &#34;AWS_ACCESS&#34;: &#34;SNIPPED&#34;, 
    &#34;INSTANCE_TYPE&#34;: &#34;c1.medium&#34;, 
    &#34;AMI&#34;: &#34;ami-6d3f9704&#34;, 
    &#34;MAX_BID&#34;: &#34;0.04&#34;,
    &#34;POST_INIT&#34;: &#34;echo \&#34;[default]\naccess_key = SNIPPED\nsecret_key = SNIPPED\&#34; &gt; /tmp/s3cfg\ncd /tmp\ns3cmd -c /tmp/s3cfg get s3://SNIPPED/GeoIPASNum.dat.gz\ns3cmd -c /tmp/s3cfg get s3://SNIPPED/GeoIP.dat.gz\ns3cmd -c /tmp/s3cfg get s3://SNIPPED/GeoLiteCity.dat.gz\ns3cmd -c /tmp/s3cfg get s3://SNIPPED/GeoIPRegion.dat.gz\ngunzip *.gz\nchown disco:disco *.dat\n\n&#34;
}
	&lt;/pre&gt;

	&lt;p&gt;This tells disposabledisco that I want a cluster with 1 master and 30 slaces all of type &lt;em&gt;c1.medium&lt;/em&gt;, and use &lt;em&gt;ami-6d3f9704&lt;/em&gt; as the starting point. It lists out the packages to be installed via apt-get and python dependencies to be installed using PIP. You can link to external tar, git repo, etc. Basically anything pip allows after &lt;em&gt;pip install&lt;/em&gt;&lt;/p&gt;

	&lt;p&gt;The &lt;em&gt;POST_INIT&lt;/em&gt; portion is bash script that runs as root after rest of the install. In my case I am downloading and uncompressing different GeoIP databases archived in a S3 bucket for use from within disco jobs.&lt;/p&gt;

	&lt;p&gt;Once the config file is ready run the following command many times. The output is fairly verbose.&lt;/p&gt;

	&lt;pre&gt;
python create_cluster.py config.json
	&lt;/pre&gt;

	&lt;p&gt;Why many times? Cause there is no state stored in the system. All state is managed using EC2 tags. This is what the script does on each run&lt;/p&gt;

	&lt;ul&gt;
		&lt;li&gt;Check if master is running. If not request a spot instance for it (and kill any zombie slaves lying around from previous runs).&lt;/li&gt;
		&lt;li&gt;
			If master us up and running.
			&lt;ul&gt;
				&lt;li&gt;Print the ssh command needed to setup port forwarding. After running the given ssh command you can see http://localhost:8090 on the browser to see disco&#39;s UI in all its glory.&lt;/li&gt;
				&lt;li&gt;print the command to export DISCO_PROXY so you can create jobs locally&lt;/li&gt;
				&lt;li&gt;Check inventory of slaves. A slave can have 3 statuses. 1) &lt;em&gt;pending&lt;/em&gt; - spot instance requested. 2) &lt;em&gt;running&lt;/em&gt; - the instance is running. 3) &lt;em&gt;bootstrapped&lt;/em&gt; - slave is completely setup and can be added to master.&lt;/li&gt;
				&lt;li&gt;If total number of slaves is less than &lt;em&gt;NUM_SLAVES&lt;/em&gt; launch the remaining&lt;/li&gt;
				&lt;li&gt;Try and bootstrap any &lt;em&gt;running&lt;/em&gt; instances. If bootstrap was successful, change the EC2 tag.&lt;/li&gt;
			&lt;/ul&gt;
		&lt;/li&gt;
		&lt;li&gt;Finally, update the master&#39;s disco config. Telling it hostnames of instances to use and number of workers.&lt;/li&gt;
		&lt;li&gt;???&lt;/li&gt;
		&lt;li&gt;Profit&lt;/li&gt;
	&lt;/ul&gt;

&lt;img src=&#34;http://i.ticdn.com/sajal/disco-cluster-small.png&#34; width=&#34;500&#34; height=&#34;377&#34; alt=&#34;Cloudwatch showing 31 instances&#34; title=&#34;Cloudwatch showing 31 instances&#34; /&gt;

	&lt;p&gt;Many steps involve EC2 provisioning spot instances, waiting for instance to get initialized, etc..&lt;/p&gt;

	&lt;p&gt;To help with shipping output to S3, I made some output classes for Disco&lt;/p&gt;

	&lt;ul&gt;
		&lt;li&gt;&lt;a href=&#34;https://gist.github.com/3919506&#34;&gt;S3Output&lt;/a&gt; - Each key, value returned creates a new file in S3 with the key as S3 key and value as String thats dumped inside it. So, one key should be yielded only once from reduce.&lt;/li&gt;
		&lt;li&gt;&lt;a href=&#34;https://gist.github.com/3975607&#34;&gt;S3LineOutput&lt;/a&gt; Similar to S3Output, but now it stores the output, and joins the output as one big file. has options for sorting, unique, etc. &lt;/li&gt;
	&lt;/ul&gt;

	&lt;p&gt;Both these functions can be configured gzip the contents before uploading.&lt;/p&gt;

	&lt;p&gt;As far as input is concerned, I send it a list of signed S3 urls. (Sidenote: It seems disco cannot handle https inputs at the moment, so I use http). A sample job run might look like.. &lt;/p&gt;

	&lt;pre style=&#34;width:500;overflow-x:scroll;&#34;&gt;
def get_urls():
    urls = []
    for k in bucket.list(prefix=&#34;processed&#34;):
        if k.name.endswith(&#34;gz&#34;):
            urls += [k.generate_url(3660).replace(&#34;https&#34;, &#34;http&#34;)]
    return urls

MyExampleJob().run(
	input=get_urls(),
	params={
		&#34;AWS_KEY&#34;: &#34;SNIP&#34;,
		&#34;AWS_SECRET&#34;: &#34;SNIP&#34;,
		&#34;BUCKET_NAME&#34;: &#34;SNIP&#34;,
		&#34;gzip&#34;: True
	},
	partitions=10,
	required_files=[&#34;s3lineoutput.py&#34;],
	reduce_output_stream=s3_line_output_stream
	).wait()

	&lt;/pre&gt;

	&lt;p&gt;Bonus - &lt;a href=&#34;https://gist.github.com/3941935&#34;&gt;MagicList&lt;/a&gt; - Memory efficient way to store and process potentially infinite lists.&lt;/p&gt;

	&lt;p&gt;We used Disco to compute numbers for a series of blogposts on &lt;a href=&#34;http://www.cdnplanet.com/&#34;&gt;CDN Planet&lt;/a&gt;. For this analysis it was painful process for me to manually launch Disco clusters, which lead me to create the helper scripts.&lt;/p&gt;
	&lt;ul&gt;
		&lt;li&gt;Part 1 : &lt;a href=&#34;http://www.cdnplanet.com/blog/google-dns-opendns-and-cdn-performance/&#34;&gt;Google DNS, OpenDNS and CDN performance&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;Part 2 : &lt;a href=&#34;http://www.cdnplanet.com/blog/which-cdns-support-edns-client-subnet/&#34;&gt;Which CDNs support edns-client-subnet?&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;Part 3 : &lt;a href=&#34;http://www.cdnplanet.com/blog/real-world-cdn-performance-googledns-opendns-users/&#34;&gt;Real-world CDN performance for Google DNS and OpenDNS users&lt;/a&gt;&lt;/li&gt;
	&lt;/ul&gt;

	&lt;h3&gt;Shameless plug&lt;/h3&gt;

	&lt;a href=&#34;http://www.turbobytes.com/&#34;&gt;&lt;strong&gt;Turbobytes, multi-CDN made easy&lt;/strong&gt;&lt;/a&gt;

	&lt;p&gt;Have your static content delivered by 6 global content delivery networks, not just 1. Turbobytes&#39; platform closely monitors CDN performance and makes sure your content is always delivered by the fastest CDN, automatically.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prospective search using python</title>
      <link>http://www.sajalkayan.com/prospective-search-using-python.html</link>
      <pubDate>Wed, 22 Jul 2009 08:56:31 &#43;0000</pubDate>
      
      <guid>http://www.sajalkayan.com/prospective-search-using-python.html</guid>
      <description>&lt;strong&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Prospective_search&#34; rel=&#34;nofollow&#34;&gt;Prospective search&lt;/a&gt;&lt;/strong&gt;, or &lt;strong&gt;persistent search&lt;/strong&gt;, is a relatively less common method of implementing search where the list of keywords is defined, and when provided a single document it determines the list of keywords applicable to it.

This is different from traditional (or &#34;retrospective&#34;) search, where many documents are stored into an indexed and when provided with a search term, the search engine returns the list of documents which best match the query.

The best real world examples would be how Google News Alerts(or IMHO categorization/clustering in Google News) works. When a new news story is found by Google, it makes more sense to run a prospective search on the news story to find which alert subscriptions (or news category) it belongs to, rather than searching for all the alerts repeatedly on their entire index.

Lucene has a &lt;a href=&#34;http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/index/memory/MemoryIndex.html&#34;&gt;MemoryIndex&lt;/a&gt; class for just this purpose, ive made a simple implementation in python using &lt;a href=&#34;http://lucene.apache.org/pylucene/&#34;&gt;pylucene&lt;/a&gt;. MemoryIndex is a special class in lucene for on-the-fly searching. It can contain only one doccument which may have more than one field. This is ideal for prospective search.

Installation and setup of pylucene is out of scope of this post... &lt;a href=&#34;http://lucene.apache.org/pylucene/documentation/install.html&#34;&gt;RTFM&lt;/a&gt;! (do note u need to edit the MakeFile)
&lt;pre lang=&#34;python&#34;&gt;import sys, os, lucene, time, threading

def ProspectiveSearch(body, terms):
    lucene.initVM(lucene.CLASSPATH)
    index = lucene.MemoryIndex()
    index.addField(&#34;content&#34;, body, lucene.StandardAnalyzer())
    parser = lucene.QueryParser(&#34;content&#34;, lucene.StandardAnalyzer())
    matches = []
    for term in terms:
        score=index.search(parser.parse(term))
        if score &amp;gt; 0:
            matches += [term]
    return matches&lt;/pre&gt;
sample usage :-
&lt;pre lang=&#34;python&#34;&gt;body = &#34;hi my name is sajal kayan&#34;
terms = [&#34;sajal&#34;, &#34;good&#34;, &#34;boy&#34;, &#34;name&#34;, &#34;sajal AND NOT kayan&#34;, &#34;sajal AND kayan&#34;]
matches = ProspectiveSearch(body, terms)&lt;/pre&gt;
In this case returns [&#39;sajal&#39;, &#39;name&#39;, &#39;sajal AND kayan&#39;]

Note:initVM() is &lt;a href=&#34;http://stackoverflow.com/questions/548493/jcc-initvm-doesnt-return-when-modwsgi-is-configured-as-daemon-mode&#34;&gt;giving problems&lt;/a&gt; on mod_wsgi

On my computer, this is the benchmark i noticed for a 244 word content.
&lt;ul&gt;
	&lt;li&gt;1,492 queries : 0.79 seconds (for whole script only 248ms for the search loop)&lt;/li&gt;
	&lt;li&gt;14,920 queries : 1.519 seconds&lt;/li&gt;
	&lt;li&gt;74,600 queries : 3.425 seconds&lt;/li&gt;
	&lt;li&gt;149,200 queries : 5.552 seconds&lt;/li&gt;
	&lt;li&gt;298,400 queries : 10.328 seconds&lt;/li&gt;
&lt;/ul&gt;
If you know a better method to achieve prospective search in python do let me know. Would also be interested to know if any RPC based search software does this.
</description>
    </item>
    
    <item>
      <title>I &lt;3 Django</title>
      <link>http://www.sajalkayan.com/i-django.html</link>
      <pubDate>Sun, 30 Nov 2008 19:42:39 &#43;0000</pubDate>
      
      <guid>http://www.sajalkayan.com/i-django.html</guid>
      <description>Last month(November) had been quite busy for me, had many things going on including :-
&lt;ul&gt;
	&lt;li&gt;&lt;strong&gt;My new Secret Project&lt;/strong&gt; : Its a new &lt;a href=&#34;http://www.djangoproject.com/&#34;&gt;Django&lt;/a&gt; based web project, if all goes as per plan I take over the world and declare windows illegal ;)&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;Visa issues&lt;/strong&gt; : Apparently the process for a Non-Immigrant visa in Thailand is not an easy route.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;Thai Political Uncertainty&lt;/strong&gt; : The drama is simply too irresistible to not follow, constant fights, arguments and lame comments from both sides (pro-govt and PAD)&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;Trip to India&lt;/strong&gt; : I had gone on a short trip to India on Social visit in the beginning of the month.&lt;/li&gt;
	&lt;li&gt;&lt;a href=&#34;http://www.thaindian.com/newsportal/tag/mumbai-terror-attack&#34;&gt;&lt;strong&gt;Mumbai Terror Attacks&lt;/strong&gt;&lt;/a&gt; : The bastards struck the Indian financial capital of Mumbai. Even though the 62 hour long ordeal is over now, the blame game drama has begun and would make major news for months to come.&lt;/li&gt;
&lt;/ul&gt;
So... apologies to my regular readers (just in case there are any besides me) just had too many things shoved into one month.

A couple of months ago I &lt;a href=&#34;/messing-with-ror.html&#34;&gt;wrote about how I had finally chosen Ruby on Rails over Django&lt;/a&gt;. Well... at the time of writing I hadnt taken a serious look at Django. Soon after making the post i decided to give Django a second look. The more i dugg, the deeper I fell in love Django and Python. I have &lt;a href=&#34;http://proteus-tech.com/&#34;&gt;Ben&lt;/a&gt; and &lt;a href=&#34;http://www.kirit.com/&#34;&gt;Kirit&lt;/a&gt; who I can regularly irriate with n00b questions, the &lt;a href=&#34;http://groups.google.com/group/django-users&#34;&gt;Django-Users group&lt;/a&gt; is quite active and the &lt;a href=&#34;http://docs.djangoproject.com/en/dev/&#34;&gt;Django documentation&lt;/a&gt; is just awesome. Django truly rocks.

About my new secret project, it is a website about -SNIP- and I aim it to be one of the authorative websites in it domains. The topic of the site is not one I am an expert in, however that shouldnt be an issue. The website is located at -SNIP- . For this project &lt;a href=&#34;http://www.archit.in/&#34;&gt;Archit&lt;/a&gt; is helping with css n stuff for free :) .

As expressed above, the website is made using Django, and is un early development stages and even while development the data entry is already in progress. It would eventually become a really huge database.

The best thing i liked about Django is that you can import the Django enviornment in any regular external script and have access to all the functions, models defined in your project. You even have access to all the django APIs within your project. See &lt;a href=&#34;http://www.cotellese.net/2007/09/27/running-external-scripts-against-django-models/&#34;&gt;this awesome post&lt;/a&gt; for more info of what im saying. I modified it a little to suit my project.
&lt;pre lang=&#34;python&#34;&gt;#for portability
project = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project)
os.environ[&#39;DJANGO_SETTINGS_MODULE&#39;] =&#39;topsecretproject.settings&#39;
from django.core.management import setup_environ
from topsecretproject import settings
from topsecretproject.app.models import model1, model2
setup_environ(settings)

data = model1.objects.all()
#... and so onn....&lt;/pre&gt;
Tools used : &lt;a href=&#34;http://www.postgresql.org/&#34;&gt;Postgres&lt;/a&gt;, &lt;a href=&#34;http://git.or.cz/&#34;&gt;git&lt;/a&gt;, &lt;a href=&#34;http://github.com/&#34;&gt;github&lt;/a&gt;, &lt;a href=&#34;http://projects.gnome.org/gedit/&#34;&gt;gedit&lt;/a&gt;, &lt;a href=&#34;http://www.activestate.com/Products/komodo_ide/&#34;&gt;Komodo IDE&lt;/a&gt;(looks nice but free trial gonna run out soon :( ), &lt;a href=&#34;http://www.ubuntu.com/&#34;&gt;Ubuntu&lt;/a&gt;(my notebook), &lt;a href=&#34;http://www.centos.org/&#34;&gt;CentOS&lt;/a&gt;(development server - crappy VPS)

Since this project is being built from the ground up, I have full control of everything. Keeping both &lt;strong&gt;usability and &lt;a href=&#34;/category/seo&#34;&gt;Search Engine Optimization&lt;/a&gt;&lt;/strong&gt; in mind and not really feeling like there is a compromise in any.

Perhaps in a couple of weeks the site would be somewhat presentable, then id discuss various aspects about the code and techniques im using in keeping the site SEOed. Its too early to talk about it at the moment.

Disclaimer : &lt;em&gt;No windows users were harmed during the production of this post.&lt;/em&gt;
</description>
    </item>
    
  </channel>
</rss>